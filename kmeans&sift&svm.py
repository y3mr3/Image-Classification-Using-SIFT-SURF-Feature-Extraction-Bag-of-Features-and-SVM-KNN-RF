# -*- coding: utf-8 -*-
"""Kmeans&Sift&Svm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C20MNLD0hhk5e7O9f06jrBpNfT9vi8Km
"""

!apt-get install -y -qq software-properties-common python-software-properties module-init-tools
!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null
!apt-get update -qq 2>&1 > /dev/null
!apt-get -y install -qq google-drive-ocamlfuse fuse
from google.colab import auth
auth.authenticate_user()
from oauth2client.client import GoogleCredentials
creds = GoogleCredentials.get_application_default()
import getpass
!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL
vcode = getpass.getpass()
!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}

!mkdir -p drive
!google-drive-ocamlfuse drive

"""**Install OpenCv**"""

!pip install opencv-python==3.4.2.16
!pip install opencv-contrib-python==3.4.2.16

import numpy as np
from sklearn import svm
from sklearn.metrics import accuracy_score

import cv2
import cv2 as cv

"""**Train Data Collection**"""

import numpy as np
import pandas as pd
from sklearn.cluster import MiniBatchKMeans
import os
filename = os.listdir('/content/drive/ICIAR2018/train/Benign')
label= [1 for i in filename]
filename=['/content/drive/ICIAR2018/train/Benign/'+i for i in filename]
data1=pd.DataFrame(list(zip(filename,label)), columns=['filename','label'])

filename = os.listdir('/content/drive/ICIAR2018/train/Normal')
label= [0 for i in filename]
filename=['/content/drive/ICIAR2018/train/Normal/'+i for i in filename]
data0=pd.DataFrame(list(zip(filename,label)), columns=['filename','label'])

filename = os.listdir('/content/drive/ICIAR2018/train/InSitu')
label= [2 for i in filename]
filename=['/content/drive/ICIAR2018/train/InSitu/'+i for i in filename]
data2=pd.DataFrame(list(zip(filename,label)), columns=['filename','label'])

filename = os.listdir('/content/drive/ICIAR2018/train/Invasive')
label= [3 for i in filename]
filename=['/content/drive/ICIAR2018/train/Invasive/'+i for i in filename]
data3=pd.DataFrame(list(zip(filename,label)), columns=['filename','label'])

"""***Test Data Collection***"""

import numpy as np
import pandas as pd
from sklearn.cluster import MiniBatchKMeans
import os

filename = os.listdir('/content/drive/ICIAR2018/test/Benign')
label= [1 for i in filename]
filename=['/content/drive/ICIAR2018/test/Benign/'+i for i in filename]
test1=pd.DataFrame(list(zip(filename,label)), columns=['filename','label'])

filename = os.listdir('/content/drive/ICIAR2018/test/Normal')
label= [0 for i in filename]
filename=['/content/drive/ICIAR2018/test/Normal/'+i for i in filename]
test0=pd.DataFrame(list(zip(filename,label)), columns=['filename','label'])

filename = os.listdir('/content/drive/ICIAR2018/test/InSitu')
label= [2 for i in filename]
filename=['/content/drive/ICIAR2018/test/InSitu/'+i for i in filename]
test2=pd.DataFrame(list(zip(filename,label)), columns=['filename','label'])

filename = os.listdir('/content/drive/ICIAR2018/test/Invasive')
label= [3 for i in filename]
filename=['/content/drive/ICIAR2018/test/Invasive/'+i for i in filename]
test3=pd.DataFrame(list(zip(filename,label)), columns=['filename','label'])

import pandas as pd
data=pd.concat([data0,data1,data2,data3],ignore_index=True)

test=pd.concat([test0,test1,test2,test3],ignore_index=True)

"""**Bu Fonksiyon Train Data Setine SIFT Uygularak Elde Edilen Keypoint ve Descriptorleri K-means'le Kümeleyerek Model Databesini Oluşturur.**"""

def read_and_clusterize(df, num_cluster):

    sift_keypoints = []
    for index, row in data.iterrows():
       image = cv2.imread(row.filename,1)
       # Convert them to grayscale
       image =cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)
       # SIFT extraction
       sift = cv2.xfeatures2d.SIFT_create()
       #append the descriptors to a list of descriptors#append the descriptors to a list of descriptors
       kp, descriptors = sift.detectAndCompute(image,None)
       sift_keypoints.append(descriptors)
    sift_keypoints=np.asarray(sift_keypoints)
    sift_keypoints=np.concatenate(sift_keypoints, axis=0)
    #with the descriptors detected, lets clusterize them
    print("Training kmeans")    
    kmeans = MiniBatchKMeans(n_clusters=num_cluster, random_state=0).fit(sift_keypoints)
    #return the learned model
    return kmeans

from sklearn.externals import joblib

def save_model(filename):
  path='/content/drive/ICIAR2018/'+filename
  joblib.dump(model, path)
  print("Model Succesfully Saved")

def load_model(filename):
  path='/content/drive/ICIAR2018/'+filename
  return joblib.load(path)

"""**Bu Fonksiyonda Görüntülerin Kmeans ile Histogramı Çıkarılıp Histogramdan Feature Vektörler Elde Edilirken Etiket Atamasıda Burda Yapılır.**"""

def calculate_centroids_histogram(df, model,num_clusters):

    feature_vectors=[]
    class_vectors=[]
    for index,row in df.iterrows():
        #read image
        image = cv2.imread(row.filename,1)
        #Convert them to grayscale
        image =cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)
        #SIFT extraction
        sift = cv2.xfeatures2d.SIFT_create()
        kp, descriptors = sift.detectAndCompute(image,None)
        #classification of all descriptors in the model
        predict_kmeans=model.predict(descriptors)
        #calculates the histogram
        hist, bin_edges=np.histogram(predict_kmeans, bins=num_clusters)
        #hist, bin_edges=np.histogram(predict_kmeans)
        #histogram is the feature vector
        feature_vectors.append(hist)
        #define the class of the image (elephant or electric guitar)
        #class_sample=define_class(row.label)
        class_sample=row.label
        class_vectors.append(class_sample)

    feature_vectors=np.asarray(feature_vectors)
    class_vectors=np.asarray(class_vectors)
    #return vectors and classes we want to classify
    return class_vectors, feature_vectors

model=read_and_clusterize(data,200)

save_model('clusterize_model_n_clusters_200_sift')

#model = joblib.load('/content/drive/ICIAR2018/clusterize_model_n_clusters_4')

"""**Train Data Setinin Feature Vektörleri Oluşturup Etiketleniyor.**"""

print("Step 2: Extracting histograms of training and testing images")
print("Training")
[train_class,train_featvec]=calculate_centroids_histogram(data,model,1000)

"""**Train Data Setinin Feature Vektörleri Oluşturup Etiketleniyor.**"""

print("Testing")
[test_class,test_featvec]=calculate_centroids_histogram(test,model,1000)

print("Step 3: Training the SVM classifier")
clf = svm.SVC(gamma='auto',kernel='linear')
clf.fit(train_featvec, train_class)

print("Step 4: Testing the SVM classifier")  
predict=clf.predict(test_featvec)

score=accuracy_score(np.asarray(test_class), predict)

file_object  = open('/content/drive/ICIAR2018/results.txt', "a")
file_object.write("%f\n" % score)
file_object.close()

print("Accuracy:" +str(score))

predict

def Dataset_single_load(path,model):
  feature_vectors=[]
  image = cv2.imread(path,1)
  image =cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)
  sift = cv2.xfeatures2d.SIFT_create()
  kp, descriptors = sift.detectAndCompute(image,None)
  predict_kmeans=model.predict(descriptors)
  hist, bin_edges=np.histogram(predict_kmeans,bins=1000)
  feature_vectors.append(hist)
  return feature_vectors
  #feature_vectors=np.asarray(feature_vectors)
  #predict=clf.predict(test_featvec)
  #return predict

"""Sınıfını öğrenmek istediğiniz görüntünün path'ini giriniz"""

s= Dataset_single_load('/content/drive/ICIAR2018/train/Invasive/iv061.tif',model)
predict=clf.predict(s)
predict

s= Dataset_single_load('/content/drive/ICIAR2018/train/Normal/n076.tif',model)
predict=clf.predict(s)
predict